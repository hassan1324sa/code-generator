{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white; \">\n",
    "    importing libraries\n",
    "</h1>\n",
    "<ul style=\"color:white; font-size:20px;\">\n",
    "<li> re : for data cleaning and preprocessing</li>\n",
    "<li> pandas : for data manipulation and analysis</li>\n",
    "<li> torch : for building and training deep learning models</li>\n",
    "<li> DataLoader  : for loading and preprocessing data</li>\n",
    "<li> torch.nn : for building and training neural networks</li>\n",
    "<li> datasets : for getting data from Hugging Face</li>\n",
    "<li> build_vocab_from_iterator : for building vocabulary from text data</li>\n",
    "<li> spacy : for tokenizing text </li>\n",
    "<li> IPython.display  : for displaying  code</li>\n",
    "<li> tokenize  : for tokenizing code</li>\n",
    "<li> BytesIO  : for encoding code</li>\n",
    "<li> torch.nn : for create decoder</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan/anaconda3/envs/Ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display, Markdown\n",
    "import tokenize\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;\">\n",
    "data preprocessing\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<a href=\"https://huggingface.co/datasets/nceyda/YAP470_Code_Generation_Dataset\">dataset link</a>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"nceyda/YAP470_Code_Generation_Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "        num_rows: 74698\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "        num_rows: 13182\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "        num_rows: 9765\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```ndiff\n",
       "      # Some users have more than 1 default template.\n",
       "      # This shouldn't be possible, make sure is will be just 1.\n",
       "      User = apps.get_model('users', 'LilyUser')\n",
       "      DefaultEmailTemplate = apps.get_model('email', 'DefaultEmailTemplate')\n",
       "  \n",
       "-     print('\\nFixing default template for the following users:')\n",
       "```\n",
       "\n",
       "```ndiff\n",
       "              # User has more than one default template.\n",
       "              # Best guess would be that the user prefers the last set template to be the default.\n",
       "              # So remove all except the last one.\n",
       "              template_to_keep = templates.last()\n",
       "              templates.exclude(id=template_to_keep.id).delete()\n",
       "- \n",
       "-             print('%d:\\t%s' % (user.pk, user.email))\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('I\\'d like to see a change in the code that achieves this: \"Remove print statements, not usefull anymore.\"\\nFor your reference, this is the current state of lily/messaging/email/migrations/0013_fix_multple_default_templates.py:\\n\\n```python\\n# -*- coding: utf-8 -*-\\nfrom __future__ import unicode_literals\\n\\nfrom django.db import models, migrations\\n\\n\\ndef fix_multiple_default_templates(apps, schema_editor):\\n    # Some users have more than 1 default template.\\n    # This shouldn\\'t be possible, make sure is will be just 1.\\n    User = apps.get_model(\\'users\\', \\'LilyUser\\')\\n    DefaultEmailTemplate = apps.get_model(\\'email\\', \\'DefaultEmailTemplate\\')\\n\\n    print(\\'\\\\nFixing default template for the following users:\\')\\n    for user in User.objects.all():\\n        templates = DefaultEmailTemplate.objects.filter(user=user.pk).order_by(\\'id\\')\\n        if templates.count() > 1:\\n            # User has more than one default template.\\n            # Best guess would be that the user prefers the last set template to be the default.\\n            # So remove all except the last one.\\n            template_to_keep = templates.last()\\n            templates.exclude(id=template_to_keep.id).delete()\\n\\n            print(\\'%d:\\\\t%s\\' % (user.pk, user.email))\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        (\\'email\\', \\'0012_auto_20160715_1423\\'),\\n    ]\\n\\n    operations = [\\n        migrations.RunPython(fix_multiple_default_templates),\\n    ]\\n\\n```',\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ds[\"train\"][\"input\"][:500]\n",
    "outputs = ds[\"train\"][\"output\"][:500]\n",
    "\n",
    "inputs[-2],display(Markdown(outputs[-2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9<=>\\n\\\\u0600-\\\\u06FF\\\\u4e00-\\\\u9fff_.,:()\\[\\]{} \\\\]', '', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCode(code):\n",
    "    lines = code.strip().split('\\n')\n",
    "    \n",
    "    if not lines[0].startswith('```python'):\n",
    "        lines.insert(0, '```python')\n",
    "    \n",
    "    if not lines[-1].endswith('```'):\n",
    "        lines.append('```')\n",
    "    \n",
    "    return '\\n'.join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeCode(code):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for tok in tokenize.tokenize(BytesIO(code.encode('utf-8')).readline):\n",
    "            if tok.type not in {tokenize.ENCODING, tokenize.COMMENT}:\n",
    "                tokens.append(tok.string.strip())\n",
    "    except:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenizeText(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yieldTokens(data, tokenizer):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def buildVocab(data, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yieldTokens(data, tokenizer),\n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "        min_freq=2\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedInputs = [cleanText(text) for text in inputs]\n",
    "cleanedOutputs = [cleanCode(cleanText(text)) for text in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n\"\"\"Templatetags for djangopress.\"\"\"\\nfrom datetime import date\\nfrom collections import defaultdict\\n\\nfrom django import template\\n\\nfrom djangopress.models import Post, Category\\n\\n\\nregister = template.Library()\\n\\n\\n@register.inclusion_tag(\\'djangopress/tags/archive_list.html\\')\\ndef archive_list():\\n    \"\"\"List post by date\"\"\"\\n    posts = Post.objects.all()\\n    years_dictionary = defaultdict(set)\\n    for post in posts:\\n        year = post.creation_date.year\\n        month = post.creation_date.month\\n        years_dictionary[year].add(month)\\n    years = {}\\n    for year, months in years_dictionary.items():\\n        year = str(year)\\n        years[year] = []\\n        for month in months:\\n            years[year].append(date(int(year), month, 1))\\n    for year in years:\\n        years[year].sort(reverse=True)\\n    return {\\'years\\': years}\\n\\n\\n@register.inclusion_tag(\\'djangopress/tags/category_list.html\\')\\ndef category_list():\\n    \"\"\"List the categories in the blog.\"\"\"\\n    categories = Category.objects.all()\\n    return {\\'categories\\': categories}\\n\\n```'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "python\n",
       "Templatetags for djangopress.\n",
       "from datetime import date\n",
       "from collections import defaultdict\n",
       "\n",
       "from django import template\n",
       "\n",
       "from djangopress.models import Post, Category\n",
       "\n",
       "\n",
       "register = template.Library()\n",
       "\n",
       "\n",
       "@register.inclusion_tag(djangopresstagsarchive_list.html)\n",
       "def archive_list():\n",
       "    List post by date\n",
       "    posts = Post.objects.all()\n",
       "    years_dictionary = defaultdict(set)\n",
       "    for post in posts:\n",
       "        year = post.creation_date.year\n",
       "        month = post.creation_date.month\n",
       "        years_dictionary[year].add(month)\n",
       "    years = {}\n",
       "    for year, months in years_dictionary.items():\n",
       "        year = str(year)\n",
       "        years[year] = []\n",
       "        for month in months:\n",
       "            years[year].append(date(int(year), month, 1))\n",
       "    for year in years:\n",
       "        years[year].sort(reverse=True)\n",
       "    return {years: years}\n",
       "\n",
       "\n",
       "@register.inclusion_tag(djangopresstagscategory_list.html)\n",
       "def category_list():\n",
       "    List the categories in the blog.\n",
       "    categories = Category.objects.all()\n",
       "    return {categories: categories}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(cleanedOutputs[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "for key, value in my_dict.items():\n",
       "    if my_dict.values().count(value) > 1:\n",
       "        print(key, value)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(cleanedOutputs[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabText = buildVocab(cleanedInputs, tokenizeText)\n",
    "vocabCode = buildVocab(cleanedOutputs,tokenizeCode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcess(Text, Code, seqLen=None):\n",
    "    data = []\n",
    "    for rawText, rawCode in zip(Text, Code):\n",
    "        tokensText = [vocabText['<sos>']] + [vocabText[token] for token in tokenizeText(rawText)] + [vocabText['<eos>']]\n",
    "        tokensCode = [vocabCode['<sos>']] + [vocabCode[token] for token in tokenizeCode(rawCode)] + [vocabCode['<eos>']]\n",
    "        \n",
    "        if seqLen is not None:\n",
    "            if len(tokensText) > seqLen:\n",
    "                tokensText = tokensText[:seqLen]\n",
    "            else:\n",
    "                tokensText += [vocabText['<pad>']] * (seqLen - len(tokensText))\n",
    "            \n",
    "            if len(tokensCode) > seqLen:\n",
    "                tokensCode = tokensCode[:seqLen]\n",
    "            else:\n",
    "                tokensCode += [vocabCode['<pad>']] * (seqLen - len(tokensCode))\n",
    "        \n",
    "        data.append((torch.tensor(tokensText, dtype=torch.long),torch.tensor(tokensCode, dtype=torch.long)))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = dataProcess(inputs,outputs,500)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else  'cpu')\n",
    "batchSize = 32\n",
    "trainLoader = DataLoader(\n",
    "    trainData, \n",
    "    batch_size=batchSize,\n",
    "    shuffle=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\">\n",
    "<h1>\n",
    "create decoder only from transformer model\n",
    "</h1>\n",
    "<h3> why we need decoder only from transformer model </h3>\n",
    "<p>\n",
    "The Transformer model is a popular architecture for sequence-to-sequence tasks, such as machine translation, text\n",
    "generation, and question answering. It consists of an encoder and a decoder. The encoder takes in a\n",
    "sequence of tokens ( words or characters) and produces a continuous representation of the input sequence. \n",
    "The decoder then takes this representation and generates the output sequence, one token at a time.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, inputVocabSize, outputVocabSize, sequenceLen, embedSize, hiddenSize, numLayers, numHeads, dropout=0.1):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        self.srcEmbedding = nn.Embedding(inputVocabSize, embedSize)\n",
    "        \n",
    "        self.tgtEmbedding = nn.Embedding(outputVocabSize, embedSize)\n",
    "        self.positionalEncoding = nn.Parameter(torch.zeros(1, sequenceLen, embedSize))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=numLayers)\n",
    "        self.fc_out = nn.Linear(embedSize, outputVocabSize)\n",
    "        \n",
    "    def forward(self, tgt, src, tgtMask=None):\n",
    "        \n",
    "        srcEmb = self.srcEmbedding(src) + self.positionalEncoding[:, :src.size(1), :]\n",
    "        srcEmb = srcEmb.permute(1, 0, 2)  # (seq_len, batch, embed_size)\n",
    "        \n",
    "        tgtEmb = self.tgtEmbedding(tgt) + self.positionalEncoding[:, :tgt.size(1), :]\n",
    "        tgtEmb = tgtEmb.permute(1, 0, 2)  # (seq_len, batch, embed_size)\n",
    "        \n",
    "        output = self.decoder(tgtEmb, srcEmb, tgt_mask=tgtMask)\n",
    "        output = output.permute(1, 0, 2)  # (batch, seq_len, embed_size)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "inputVocabSize = len(vocabText)\n",
    "outputVocabSize = len(vocabCode)\n",
    "embedSize = 512\n",
    "hiddenSize = 2048\n",
    "numLayers = 6\n",
    "numHeads = 8\n",
    "dropout = 0.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderModel(\n",
      "  (srcEmbedding): Embedding(5925, 512)\n",
      "  (tgtEmbedding): Embedding(3321, 512)\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "        (dropout3): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=3321, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoderModel = TransformerDecoderModel(inputVocabSize,outputVocabSize,500, embedSize, hiddenSize, numLayers, numHeads, dropout)\n",
    "print(decoderModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31917817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalParams = sum(p.numel() for p in decoderModel.parameters())\n",
    "totalParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Train Loss: 5.9366, Valid Loss: 4.7033\n",
      "Validation loss improved Saving best model\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabCode['<pad>'])\n",
    "optimizer = torch.optim.Adam(decoderModel.parameters(), lr=0.0001)\n",
    "\n",
    "validInputs = ds[\"validation\"][\"input\"][:500]\n",
    "validOutputs = ds[\"validation\"][\"output\"][:500]\n",
    "\n",
    "validData = dataProcess(validInputs, validOutputs, 500)\n",
    "validLoader = DataLoader(validData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "patience = 10\n",
    "bestValidLoss = float('inf')\n",
    "patienceCounter = 0\n",
    "bestModelWeights = None\n",
    "\n",
    "decoderModel = decoderModel.to(device)\n",
    "\n",
    "numEpochs = 1 \n",
    "for epoch in range(numEpochs):\n",
    "    decoderModel.train()\n",
    "    totalLoss = 0\n",
    "    \n",
    "    for batch_idx, (srcSeq, tgtSeq) in enumerate(trainLoader):\n",
    "        srcSeq = srcSeq.to(device)\n",
    "        tgtSeq = tgtSeq.to(device)\n",
    "        \n",
    "        tgtInput = tgtSeq[:, :-1]\n",
    "        tgtOutput = tgtSeq[:, 1:]\n",
    "\n",
    "        tgtMask = nn.Transformer.generate_square_subsequent_mask(tgtInput.size(1)).to(device)\n",
    "        \n",
    "        outputs = decoderModel(tgt=tgtInput, src=srcSeq, tgtMask=tgtMask)\n",
    "        \n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), tgtOutput.contiguous().view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoderModel.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "\n",
    "    decoderModel.eval()\n",
    "    validLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for srcSeq, tgtSeq in validLoader:\n",
    "            srcSeq = srcSeq.to(device)\n",
    "            tgtSeq = tgtSeq.to(device)\n",
    "            \n",
    "            tgtInput = tgtSeq[:, :-1]\n",
    "            tgtOutput = tgtSeq[:, 1:]\n",
    "            \n",
    "            tgtMask = nn.Transformer.generate_square_subsequent_mask(tgtInput.size(1)).to(device)\n",
    "            \n",
    "            outputs = decoderModel(tgt=tgtInput, src=srcSeq, tgtMask=tgtMask)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgtOutput.contiguous().view(-1))\n",
    "            validLoss += loss.item()\n",
    "    \n",
    "    avgTrainLoss = totalLoss / len(trainLoader)\n",
    "    avgValidLoss = validLoss / len(validLoader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{numEpochs}] Train Loss: {avgTrainLoss:.4f}, Valid Loss: {avgValidLoss:.4f}')\n",
    "\n",
    "    if avgValidLoss < bestValidLoss:\n",
    "        bestValidLoss = avgValidLoss\n",
    "        patienceCounter = 0\n",
    "        bestModelWeights = decoderModel.state_dict().copy()\n",
    "        torch.save(decoderModel.state_dict(),\"model.pth\")\n",
    "        print(f\"Validation loss improved Saving best model\")\n",
    "    else:\n",
    "        patienceCounter += 1\n",
    "        print(f\"Validation loss didn't improve. Patience: {patienceCounter}/{patience}\")\n",
    "        \n",
    "    if patienceCounter >= patience:\n",
    "        print(f\"Early stopping  after {epoch+1} epochs\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model weights from early stopping\n"
     ]
    }
   ],
   "source": [
    "# Load best model weights\n",
    "if bestModelWeights is not None:\n",
    "    decoderModel.load_state_dict(bestModelWeights)\n",
    "    print(\"Loaded best model weights from early stopping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamSearchDecode(model, srcInput, vocabCode, device, maxLen=500, beamWidth=5,):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        srcEmb = model.srcEmbedding(srcInput) + model.positionalEncoding[:, :srcInput.size(1), :]\n",
    "        srcEmb = srcEmb.permute(1, 0, 2)  # (seq_len, 1, embed_size)\n",
    "    \n",
    "    beams = [{\n",
    "        'sequence': [vocabCode['<sos>']],\n",
    "        'score': 0.0\n",
    "    }]\n",
    "\n",
    "    for _ in range(maxLen):\n",
    "        candidates = []\n",
    "        for beam in beams:\n",
    "            if not beam['sequence']:\n",
    "                continue\n",
    "            tgt = torch.tensor(beam['sequence'], device=device).unsqueeze(0)  # (1, seq_len)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                tgtEmb = model.tgtEmbedding(tgt) + model.positionalEncoding[:, :tgt.size(1), :]\n",
    "                tgtEmb = tgtEmb.permute(1, 0, 2)  # (seq_len, 1, embed_size)\n",
    "                \n",
    "                seq_len = tgt.size(1)\n",
    "                tgtMask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "                \n",
    "                output = model.decoder(tgtEmb, srcEmb, tgt_mask=tgtMask)\n",
    "                output = output.permute(1, 0, 2)  # (1, seq_len, embed_size)\n",
    "                logits = model.fc_out(output[:, -1, :])\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            topProbs, topIndices = torch.topk(probs, beamWidth * 2, dim=-1)\n",
    "            for i in range(topProbs.size(-1)):\n",
    "                newToken = topIndices[0][i].item()\n",
    "                newScore = beam['score'] - torch.log(topProbs[0][i]).item()\n",
    "                candidates.append({\n",
    "                    'sequence': beam['sequence'] + [newToken],\n",
    "                    'score': newScore\n",
    "                })\n",
    "        \n",
    "        candidates = sorted(candidates, key=lambda x: x['score'])[:beamWidth]\n",
    "        beams = candidates\n",
    "        \n",
    "        if all(beam['sequence'][-1] == vocabCode['<eos>'] for beam in beams):\n",
    "            break\n",
    "\n",
    "    bestSequence = beams[0]['sequence']\n",
    "    return [vocabCode.get_itos()[idx] for idx in bestSequence \n",
    "            if idx not in {vocabCode['<sos>'], vocabCode['<eos>'],vocabCode[\"<unk>\"]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Incorporate this change into the code: Add a csv export for caching the list of beers for display in the app\\nFor your reference, this is the current state of builder.py:\\n\\npython\\nimport ratebeer\\nimport string\\n\\n\\ndef strip_brewery_name(brewery_name, beer_name):\\n    brewery_word_list = brewery_name.split()\\n    for word in brewery_word_list:\\n        beer_name = beer_name.replace(word, )\\n    return beer_name.strip()\\n\\ncategories = []\\ncategories.append(09)\\nfor letter in string.ascii_uppercase:\\n    categories.append(letter)\\n\\nrb = ratebeer.RateBeer()\\n\\nwith open(eng.user_words,w) as f:\\n    for category in categories:\\n        brewery_list = rb.brewers_by_alpha(category)\\n        for brewery in brewery_list:\\n            beer_list = brewery.get_beers()\\n            for beer in beer_list:\\n                index the beer name without the bewery too\\n                beer_name_without_brewery = strip_brewery_name(brewery.name, beer.name)\\n                f.writelines(beer_name_without_brewery.encode(utf8)  \\\\n)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedInputs[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\npython\\nThis module contains a cache handler.\\n\\n__author__ = Aaron Steele\\n\\n MOL imports\\nimport cache\\n\\n Standard Python imports\\nimport json\\nimport logging\\nimport urllib\\nimport webapp2\\n\\n Google App Engine imports\\nfrom google.appengine.api import urlfetch\\nfrom google.appengine.ext.webapp.util import run_wsgi_app\\n\\nclass GetHandler(webapp2.RequestHandler):\\n    Request handler for cache requests.\\n\\n    def post(self):\\n        Returns a cached value by key or None if it doesnt exist.\\n        key = self.request.get(key, empty)\\n        logging.info(SEARCH_KEY=s  key)\\n        sql = self.request.get(sql, None)\\n        cache_buster = self.request.get(cache_buster, None)\\n        if not cache_buster:\\n            value = cache.get(key)\\n        if not value and sql:\\n            url = http:mol.cartodb.comapiv2sql?s  urllib.urlencode(dict(q=sql))\\n            value = urlfetch.fetch(url, deadline=60).content\\n            if not json.loads(value).has_key(error) and not cache_buster:\\n                cache.add(key, value)\\n        self.response.headers[ContentType] = applicationjson\\n        self.response.out.write(value)\\n\\napplication = webapp2.WSGIApplication(\\n    [(cacheget, GetHandler),],\\n    debug=True)\\n\\ndef main():\\n    run_wsgi_app(application)\\n\\nif __name__ == __main__:\\n    main()\\n```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedOutputs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n"
     ]
    }
   ],
   "source": [
    "srcText = \"can you create function to get the first element of a list\"\n",
    "srcTokens = [vocabText[token] for token in tokenizeText(srcText)]\n",
    "srcInput = torch.tensor([vocabText['<sos>']] + srcTokens + [vocabText['<eos>']]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate code \n",
    "generated = beamSearchDecode(\n",
    "    model=decoderModel,\n",
    "    srcInput=srcInput,\n",
    "    vocabCode=vocabCode,\n",
    "    device=device,\n",
    "    beamWidth=5,\n",
    "    maxLen=500,\n",
    ")\n",
    "\n",
    "print(''.join(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
